{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c681b7e-10af-4fba-a15f-92115337ae0e",
   "metadata": {},
   "source": [
    "# Assignment 1 - Exercise Setup\n",
    "This notebook contains the necessary code setup for the accompanying exercises. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f6f012-4fa1-4b82-833a-870142736b08",
   "metadata": {},
   "source": [
    "# 1. Coordinate Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b48384e-2d95-4fa9-8780-0362e64f74d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "2.1666666666666665\n",
      "2.25\n",
      "26.66666666666666\n",
      "9.555555555555554\n",
      "12.83333333333333\n",
      "8.154845485377136\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def argmin_x1(x):\n",
    "      return x[2] + (3/2)*x[1] - 0.5\n",
    "\n",
    "def argmin_x2(x):\n",
    "    return (1/6)*(x[0] + 2*x[2] + 5)\n",
    "\n",
    "def argmin_x3(x):\n",
    "    return (1/4)*(x[0] + 3*x[1] - 4)\n",
    "\n",
    "def f(x):\n",
    "    return np.exp(x[0] - 3*x[1] + 3) + \\\n",
    "           np.exp(3*x[1] - 2*x[2] - 2) + \\\n",
    "           np.exp(2*x[2] - x[0] + 2)\n",
    "\n",
    "def coordinate_descent(f, argmin, x0, max_iter=100, verbose=False):\n",
    "    x = np.array(x0, dtype=float)\n",
    "    # to track the trajectory of x\n",
    "    history = [x.copy()]\n",
    "\n",
    "    for t in range(max_iter):\n",
    "        for i in range(len(x)):\n",
    "             # in-place update\n",
    "            x[i] = argmin[i](x)\n",
    "        history.append(x.copy())\n",
    "\n",
    "    return x\n",
    "\n",
    "#1a\n",
    "x_0 = [4, 3, 2]\n",
    "print(argmin_x1(x_0))\n",
    "print(argmin_x2(x_0))\n",
    "print(argmin_x3(x_0))\n",
    "\n",
    "#1b\n",
    "x_0 = [1, 20, 5]\n",
    "argmin = [argmin_x1, argmin_x2, argmin_x3]\n",
    "x = coordinate_descent(f, argmin, x_0, max_iter=25)\n",
    "\n",
    "print(x[0])\n",
    "print(x[1])\n",
    "print(x[2])\n",
    "print(f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b708ad-29ab-44ea-881d-e6531045d6a4",
   "metadata": {},
   "source": [
    "# 2. Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5cca616e-c4c2-44c8-8b91-585128f47636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030757893214651656\n",
      "0.030757893214651656\n",
      "14.65275553453456\n",
      "14.65275553453456\n",
      "2.589840551681308\n",
      "2.589840551681308\n",
      "96.45134523450776\n",
      "[-3.14964972 -0.70105694]\n",
      "0.3093895537566177\n",
      "[2.90366768 2.09454269]\n",
      "11.238036094556296\n",
      "[ 3.44957105 -0.4745142 ]\n",
      "5.8598722120101145\n",
      "[-2.34403066  3.09835026]\n",
      "0.028026062567519532\n",
      "[2.98173939 2.04249722]\n"
     ]
    }
   ],
   "source": [
    "def f(u, v):\n",
    "    return (u**2 + v - 11)**2 + (u + v**2 - 7)**2\n",
    "\n",
    "def grad_f(u, v):\n",
    "    df_du = 2 * (u**2 + v - 11) * (2*u) + 2 * (u + v**2 - 7)\n",
    "    df_dv = 2 * (u**2 + v - 11) + 2 * (u + v**2 - 7) * (2*v)\n",
    "    return np.array([df_du, df_dv])\n",
    "\n",
    "def gradient_descent(f, grad_f, eta, u0, v0, max_iter=100) -> tuple[list, list]:\n",
    "    x0 = [u0, v0]\n",
    "    x = np.array(x0, dtype=float)\n",
    "    path = [x0]\n",
    "    values = [f(x[0], x[1])]\n",
    "    for t in range(1, max_iter):\n",
    "        eta_t = eta(t)\n",
    "        g = grad_f(x[0], x[1])\n",
    "        x = x - eta_t * g\n",
    "        path.append(x)\n",
    "        values.append(f(x[0], x[1]))\n",
    "    return values, path\n",
    "\n",
    "def eta_const(t,c=1e-3) -> float:\n",
    "    return lambda t: c\n",
    "\n",
    "def eta_sqrt(t,c=1e-3) -> float:\n",
    "    return  lambda t:( c / np.sqrt(t + 1) )\n",
    "\n",
    "def eta_multistep(t, milestones=[30, 80, 100], c=1e-3, eta_init=1e-3) -> float:\n",
    "    \"\"\"\n",
    "    Multi-step: each time t passes a milestone, multiply eta_init by c.\n",
    "    E.g., if milestones=[30,80], c=0.1, eta_init=1e-3:\n",
    "      t<30: eta=1e-3\n",
    "      30<=t<80: eta=1e-4\n",
    "      80<=t: eta=1e-5\n",
    "    \"\"\"\n",
    "    drops = sum(t >= m for m in milestones)\n",
    "    return eta_init * (c ** drops)\n",
    "\n",
    "def find_min(min_f, path):\n",
    "    for i in range(1,100):\n",
    "        x = path[i]\n",
    "        f_x = f(x[0], x[1])\n",
    "        if f_x<min_f:\n",
    "            min_f = f_x\n",
    "    return min_f\n",
    "\n",
    "x_0 = (4, -5)\n",
    "max_iter = 100\n",
    "\n",
    "#2a\n",
    "values, path = gradient_descent(f, grad_f, eta_const(1e-3), x_0[0], x_0[1], max_iter)\n",
    "print(values[99])\n",
    "min_f = 10000000\n",
    "print(find_min(min_f, path))\n",
    "\n",
    "#2b\n",
    "values, path = gradient_descent(f, grad_f, eta_sqrt(1e-3), x_0[0], x_0[1], max_iter)\n",
    "print(values[99])\n",
    "min_f = 10000000\n",
    "print(find_min(min_f, path))\n",
    "\n",
    "#2c\n",
    "values, path = gradient_descent(f, grad_f, lambda t: eta_multistep(t, milestones=[30, 80, 100], c=0.1, eta_init=1e-3), x_0[0], x_0[1], max_iter)\n",
    "print(values[99])\n",
    "min_f = 10000000\n",
    "print(find_min(min_f, path))\n",
    "\n",
    "#2d\n",
    "x_0 = (-4, 0)\n",
    "values, path = gradient_descent(f, grad_f, eta_const(1e-3), x_0[0], x_0[1], max_iter)\n",
    "print(values[99])\n",
    "print(path[99])\n",
    "\n",
    "x_0 = (0, 0)\n",
    "values, path = gradient_descent(f, grad_f, eta_const(1e-3), x_0[0], x_0[1], max_iter)\n",
    "print(values[99])\n",
    "print(path[99])\n",
    "\n",
    "x_0 = (4, 0)\n",
    "values, path = gradient_descent(f, grad_f, eta_const(1e-3), x_0[0], x_0[1], max_iter)\n",
    "print(values[99])\n",
    "print(path[99])\n",
    "\n",
    "x_0 = (0, 4)\n",
    "values, path = gradient_descent(f, grad_f, eta_const(1e-3), x_0[0], x_0[1], max_iter)\n",
    "print(values[99])\n",
    "print(path[99])\n",
    "\n",
    "x_0 = (5, 5)\n",
    "values, path = gradient_descent(f, grad_f, eta_const(1e-3), x_0[0], x_0[1], max_iter)\n",
    "print(values[99])\n",
    "print(path[99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e9ced5-f4e5-487b-9a09-cf9686eb3015",
   "metadata": {},
   "source": [
    "# 3. Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "698f827c-0cbe-458f-b430-8a5370ef43ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T11:35:49.708529Z",
     "start_time": "2025-04-28T11:35:49.652139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 20640\n",
      "\n",
      ":Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      ":Attribute Information:\n",
      "    - MedInc        median income in block group\n",
      "    - HouseAge      median house age in block group\n",
      "    - AveRooms      average number of rooms per household\n",
      "    - AveBedrms     average number of bedrooms per household\n",
      "    - Population    block group population\n",
      "    - AveOccup      average number of household members\n",
      "    - Latitude      block group latitude\n",
      "    - Longitude     block group longitude\n",
      "\n",
      ":Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "A household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns may take surprisingly large values for block groups with few households\n",
      "and many empty houses, such as vacation resorts.\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. rubric:: References\n",
      "\n",
      "- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "  Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = housing.frame\n",
    "\n",
    "data = housing.data\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(df['MedInc'], df['target'], alpha=0.5)\n",
    "# plt.show()\n",
    "\n",
    "# View the first few rows\n",
    "df.head()\n",
    "print(housing.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "685cd76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.5558915986952444\n",
      "β_MedInc = 0.85238168814851\n",
      "β_AveBedrms = 0.371131882848623\n",
      "β_HouseAge= 0.12238223843286627\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Let's separate features and target for clarity\n",
    "features = df.drop(columns=[\"MedHouseVal\"])\n",
    "target = df[\"MedHouseVal\"]\n",
    "\n",
    "# Standardize the features\n",
    "# Your Code Here\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Split the dataset (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(scaled_features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Polynomial features\n",
    "DEGREE = 1 #1 for linear, 2 for 2nd degree polynomial\n",
    "poly = PolynomialFeatures(DEGREE)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_val_poly = poly.transform(X_val)\n",
    "\n",
    "#Equation from lecture\n",
    "XTX = np.matmul(X_train_poly.T, X_train_poly)\n",
    "XTy = np.matmul(X_train_poly.T, y_train)\n",
    "b = np.linalg.inv(XTX) @ XTy\n",
    "\n",
    "# calculate MSE\n",
    "y_pred = np.matmul(X_val_poly, b)\n",
    "mse = np.mean((y_val - y_pred) ** 2)\n",
    "print(\"MSE:\", mse)\n",
    "# Get feature names\n",
    "names = poly.get_feature_names_out(features.columns)\n",
    "# dictionary: {feature_name: coefficient}\n",
    "coef_dict = dict(zip(names, b))\n",
    "\n",
    "if DEGREE == 2:\n",
    "    print(\"β_MedInc =\", coef_dict[\"MedInc\"])\n",
    "    print(\"β_MedInc:AveBedrms =\", coef_dict[\"MedInc AveBedrms\"])\n",
    "    print(\"β_HouseAge:AveBedrms =\", coef_dict[\"HouseAge AveBedrms\"])\n",
    "elif DEGREE == 1:\n",
    "    print(\"β_MedInc =\", coef_dict[\"MedInc\"])\n",
    "    print('β_AveBedrms =', coef_dict[\"AveBedrms\"])\n",
    "    print('β_HouseAge=', coef_dict[\"HouseAge\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8fd7d772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Ridge): 0.4642185754423943\n",
      "β_MedInc = 0.9345539556201539\n",
      "β_MedInc:AveBedrms = -0.13111294707252608\n",
      "β_HouseAge:AveBedrms = 0.057220670341313395\n"
     ]
    }
   ],
   "source": [
    "#3D\n",
    "# The given objective is convexsince it is the sum of convex functions.\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import numpy as np\n",
    "\n",
    "# Load and prepare data\n",
    "df = fetch_california_housing(as_frame=True).frame\n",
    "features = df.drop(columns=[\"MedHouseVal\"])\n",
    "target = df[\"MedHouseVal\"]\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Polynomial transformation\n",
    "poly = PolynomialFeatures(2)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_val_poly = poly.transform(X_val)\n",
    "\n",
    "# Ridge regression solution\n",
    "lambda_ = 0.001\n",
    "n = X_train_poly.shape[1]\n",
    "I = np.eye(n)\n",
    "\n",
    "XTX = X_train_poly.T @ X_train_poly\n",
    "XTy = X_train_poly.T @ y_train\n",
    "b = np.linalg.inv((1/n) * XTX + lambda_ * I) @ (XTy/n)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = X_val_poly @ b\n",
    "mse_ridge = np.mean((y_val - y_pred) ** 2)\n",
    "\n",
    "# dictionary: {feature_name: coefficient\n",
    "feature_names = poly.get_feature_names_out(features.columns)\n",
    "coef_dict = dict(zip(feature_names, b))\n",
    "\n",
    "# Output requested values\n",
    "print(\"MSE (Ridge):\", mse_ridge)\n",
    "print(\"β_MedInc =\", coef_dict[\"MedInc\"])\n",
    "print(\"β_MedInc:AveBedrms =\", coef_dict[\"MedInc AveBedrms\"])\n",
    "print(\"β_HouseAge:AveBedrms =\", coef_dict[\"HouseAge AveBedrms\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef733a3-bc89-4860-b9db-5a79e7d3237b",
   "metadata": {},
   "source": [
    "# 4. Bias and Variance\n",
    "You can calculate the results manually, or use whatever code you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b588fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias**2=np.float64(1.2099999999999997)\n",
      "variance=np.float64(2.940000000000001)\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def f_1(x):\n",
    "    return 2*x+4\n",
    "\n",
    "def f_2(x):\n",
    "    return x+0.1\n",
    "\n",
    "def f_3(x):\n",
    "    return 3*x+0.7\n",
    "\n",
    "x_0=0\n",
    "f_values = [f_1(0), f_2(0), f_3(0)]\n",
    "sample_mean = np.mean(f_values)\n",
    "# print(f\"{sample_mean=}\")\n",
    "bias = abs(sigmoid(x_0) - sample_mean)\n",
    "print(f\"{bias**2=}\")\n",
    "variance = np.mean([(sample_mean -f_d)**2 for f_d in f_values ])\n",
    "print(f\"{variance=}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deba38e-3363-41b9-ba5b-2318a9e49bb4",
   "metadata": {},
   "source": [
    "# 5. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "afc992b6-52aa-4173-a83f-cc30e512eb46",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fetch_20newsgroups\n\u001b[32m      2\u001b[39m categories = [\u001b[33m'\u001b[39m\u001b[33msci.space\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmisc.forsale\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcomp.graphics\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrec.sport.hockey\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m train = \u001b[43mfetch_20newsgroups\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m test = fetch_20newsgroups(subset=\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m, categories=categories)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(train.DESCR)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\20231347\\.conda\\envs\\cblenvironment\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\20231347\\.conda\\envs\\cblenvironment\\Lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py:320\u001b[39m, in \u001b[36mfetch_20newsgroups\u001b[39m\u001b[34m(data_home, subset, categories, shuffle, random_state, remove, download_if_missing, return_X_y, n_retries, delay)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download_if_missing:\n\u001b[32m    319\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mDownloading 20news dataset. This may take a few minutes.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m     cache = \u001b[43m_download_20newsgroups\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtwenty_home\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m20Newsgroups dataset not found\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\20231347\\.conda\\envs\\cblenvironment\\Lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py:92\u001b[39m, in \u001b[36m_download_20newsgroups\u001b[39m\u001b[34m(target_dir, cache_path, n_retries, delay)\u001b[39m\n\u001b[32m     88\u001b[39m     os.remove(archive_path)\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Store a zipped pickle\u001b[39;00m\n\u001b[32m     91\u001b[39m cache = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     train=\u001b[43mload_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlatin1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m     93\u001b[39m     test=load_files(test_path, encoding=\u001b[33m\"\u001b[39m\u001b[33mlatin1\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     94\u001b[39m )\n\u001b[32m     95\u001b[39m compressed_content = codecs.encode(pickle.dumps(cache), \u001b[33m\"\u001b[39m\u001b[33mzlib_codec\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cache_path, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\20231347\\.conda\\envs\\cblenvironment\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:189\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m global_skip_validation = get_config()[\u001b[33m\"\u001b[39m\u001b[33mskip_parameter_validation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m func_sig = signature(func)\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\20231347\\.conda\\envs\\cblenvironment\\Lib\\site-packages\\sklearn\\datasets\\_base.py:309\u001b[39m, in \u001b[36mload_files\u001b[39m\u001b[34m(container_path, description, categories, load_content, shuffle, encoding, decode_error, random_state, allowed_extensions)\u001b[39m\n\u001b[32m    307\u001b[39m data = []\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m filenames:\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     data.append(\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    311\u001b[39m     data = [d.decode(encoding, decode_error) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\20231347\\.conda\\envs\\cblenvironment\\Lib\\pathlib\\_abc.py:625\u001b[39m, in \u001b[36mPathBase.read_bytes\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    622\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    623\u001b[39m \u001b[33;03m    Open the file in bytes mode, read it, and close the file.\u001b[39;00m\n\u001b[32m    624\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    626\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m f.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\20231347\\.conda\\envs\\cblenvironment\\Lib\\pathlib\\_local.py:537\u001b[39m, in \u001b[36mPath.open\u001b[39m\u001b[34m(self, mode, buffering, encoding, errors, newline)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m    536\u001b[39m     encoding = io.text_encoding(encoding)\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['sci.space', 'misc.forsale', 'comp.graphics', 'rec.sport.hockey']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "print(train.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf884d6c",
   "metadata": {},
   "source": [
    "The classes are indicated categorically with indices from zero to two by the target vector. The target names tell us which index belongs to which class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6b9623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 1, ..., 2, 1, 0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train.target\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c49ea33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comp.graphics', 'misc.forsale', 'rec.sport.hockey', 'sci.space']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5059cc4a",
   "metadata": {},
   "source": [
    "We represent the documents in a bag of word format. That is, we create a data matrix ``D`` such that ``D[j,i]=1`` if the j-th document contains the i-th feature (word), and ``D[j,i]=0`` otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a59bb7-101e-4214-b13c-e7a7fa85f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import numpy as np\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", min_df=5,token_pattern=r\"[^\\W\\d_]+\", binary=True)\n",
    "D = vectorizer.fit_transform(train.data)\n",
    "D_test = vectorizer.transform(test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb33fb5",
   "metadata": {},
   "source": [
    "We get the allocation of feature indices to words by the following array, containing the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f65bfa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aa', 'aargh', 'ab', ..., 'zubov', 'zv', 'zyeh'], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fdff73",
   "metadata": {},
   "source": [
    "For example, the word `zubov` has the index 7383."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960bd3a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7383])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(vectorizer.get_feature_names_out() == 'zubov')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76bb289",
   "metadata": {},
   "source": [
    "We can use np.unique to find the counts of each category. We then divide each count by the total sum to get the proportion of each category (prior probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72671a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class comp.graphics: Prior = 0.247248\n",
      "Class misc.forsale: Prior = 0.247671\n",
      "Class rec.sport.hockey: Prior = 0.254022\n",
      "Class sci.space: Prior = 0.251058\n"
     ]
    }
   ],
   "source": [
    "class_labels, counts = np.unique(y_train, return_counts=True)\n",
    "priors = counts / counts.sum()\n",
    "\n",
    "for label, prior in zip(class_labels, priors):\n",
    "    print(f\"Class {train.target_names[label]}: Prior = {prior:4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf77135",
   "metadata": {},
   "source": [
    "Since the domain of the feature $x_{\"chips\"}$ is 1 and 0. We get that $|X_{\"chips\"}|=2$. From this we can calculate the log-probabilities with Laplace Smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1cb70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Class 'comp.graphics': log(p(x_chip = 1 | class)) = -4.1727\n",
      " Class 'misc.forsale': log(p(x_chip = 1 | class)) = -4.0690\n",
      " Class 'rec.sport.hockey': log(p(x_chip = 1 | class)) = -4.6052\n",
      " Class 'sci.space': log(p(x_chip = 1 | class)) = -6.3852\n"
     ]
    }
   ],
   "source": [
    "chip_index = vectorizer.vocabulary_[\"chip\"]\n",
    "alpha = 1 * 10**(-5)\n",
    "class_names = train.target_names\n",
    "chip_column = D[:, chip_index].toarray().flatten()\n",
    "for label in class_labels:\n",
    "  class_mask = y_train == label\n",
    "  n_docs_in_class = np.sum(class_mask)\n",
    "  n_chip_in_class = np.sum(chip_column[class_mask])\n",
    "\n",
    "  likelihood = (n_chip_in_class + alpha) / (n_docs_in_class + 2 * alpha)\n",
    "  log_likelihood = np.log(likelihood)\n",
    "  print(f\" Class '{class_names[label]}': log(p(x_chip = 1 | class)) = {log_likelihood:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4942321",
   "metadata": {},
   "source": [
    "Using this method and taking into account the log probabilites we can compute the posterior probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fc1f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word: 'electronics'\n",
      "  p(y=0 | x_electronics=1) = 0.2114  (comp.graphics)\n",
      "  p(y=1 | x_electronics=1) = 0.4751  (misc.forsale)\n",
      "  p(y=2 | x_electronics=1) = 0.0000  (rec.sport.hockey)\n",
      "  p(y=3 | x_electronics=1) = 0.3135  (sci.space)\n",
      "\n",
      "Word: 'sale'\n",
      "  p(y=0 | x_sale=1) = 0.0027  (comp.graphics)\n",
      "  p(y=1 | x_sale=1) = 0.9579  (misc.forsale)\n",
      "  p(y=2 | x_sale=1) = 0.0157  (rec.sport.hockey)\n",
      "  p(y=3 | x_sale=1) = 0.0238  (sci.space)\n",
      "\n",
      "Word: 'games'\n",
      "  p(y=0 | x_games=1) = 0.0301  (comp.graphics)\n",
      "  p(y=1 | x_games=1) = 0.2509  (misc.forsale)\n",
      "  p(y=2 | x_games=1) = 0.6991  (rec.sport.hockey)\n",
      "  p(y=3 | x_games=1) = 0.0199  (sci.space)\n",
      "\n",
      "Word: 'ball'\n",
      "  p(y=0 | x_ball=1) = 0.1686  (comp.graphics)\n",
      "  p(y=1 | x_ball=1) = 0.0842  (misc.forsale)\n",
      "  p(y=2 | x_ball=1) = 0.3305  (rec.sport.hockey)\n",
      "  p(y=3 | x_ball=1) = 0.4167  (sci.space)\n"
     ]
    }
   ],
   "source": [
    "words = [\"electronics\", \"sale\", \"games\", \"ball\"]\n",
    "log_likelihoods= {}\n",
    "for word in words:\n",
    "    if word in vectorizer.vocabulary_:\n",
    "        word_idx = vectorizer.vocabulary_[word]\n",
    "        word_presence = D[:, word_idx].toarray().flatten()\n",
    "        \n",
    "        log_prob_word_given_class = []\n",
    "        for label in np.unique(y_train):\n",
    "            class_mask = (y_train == label)\n",
    "            n_docs_in_class = np.sum(class_mask)\n",
    "            n_word_in_class = np.sum(word_presence[class_mask])\n",
    "            \n",
    "            prob = (n_word_in_class + alpha) / (n_docs_in_class + 2 * alpha)\n",
    "            log_prob_word_given_class.append(np.log(prob))\n",
    "        \n",
    "        log_likelihoods[word] = log_prob_word_given_class\n",
    "\n",
    "log_posteriors = {}\n",
    "for word in words:\n",
    "\n",
    "    log_joint = np.array(log_likelihoods[word]) + priors\n",
    "    log_p_word = np.logaddexp.reduce(log_joint)\n",
    "    log_posteriors[word] = log_joint - log_p_word\n",
    "\n",
    "posteriors = {word: np.exp(log_posteriors[word]) for word in words}\n",
    "\n",
    "for word in words:\n",
    "    print(f\"\\nWord: '{word}'\")\n",
    "    for label in np.unique(y_train):\n",
    "        print(\n",
    "            f\"  p(y={label} | x_{word}=1) = {posteriors[word][label]:.4f}  \"\n",
    "            f\"({class_names[label]})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98761ed-9fac-4d6c-bb84-488f9da34bf9",
   "metadata": {},
   "source": [
    "# 6. Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0813b8-94c6-44e5-84b8-d91243c2cbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                  5.1               3.5                1.4               0.2   \n",
      "1                  4.9               3.0                1.4               0.2   \n",
      "2                  4.7               3.2                1.3               0.2   \n",
      "3                  4.6               3.1                1.5               0.2   \n",
      "4                  5.0               3.6                1.4               0.2   \n",
      "..                 ...               ...                ...               ...   \n",
      "145                6.7               3.0                5.2               2.3   \n",
      "146                6.3               2.5                5.0               1.9   \n",
      "147                6.5               3.0                5.2               2.0   \n",
      "148                6.2               3.4                5.4               2.3   \n",
      "149                5.9               3.0                5.1               1.8   \n",
      "\n",
      "     target  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "..      ...  \n",
      "145       2  \n",
      "146       2  \n",
      "147       2  \n",
      "148       2  \n",
      "149       2  \n",
      "\n",
      "[150 rows x 5 columns]\n",
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['target'] = y\n",
    "print(df)\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc7320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_entropy=1.0986122886681096\n",
      "information_gain=0.18065570889289262\n"
     ]
    }
   ],
   "source": [
    "#6a\n",
    "import math\n",
    "def get_class_prob(target):\n",
    "    \"\"\"Returns a tuple (setosa_prob, versicolor_prob, virginica_prob) of target y\"\"\"\n",
    "    setosa_prob= np.count_nonzero(target==0)/len(target)\n",
    "    versicolor_prob = np.count_nonzero(target==1)/len(target)\n",
    "    virginica_prob = np.count_nonzero(target==2)/len(target)\n",
    "    return setosa_prob, versicolor_prob, virginica_prob\n",
    "\n",
    "def entropy(y):\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    classes_prob = get_class_prob(y)\n",
    "    \"\"\"Calcs entropy of tuple (setosa_prob, versicolor_prob, virginica_prob)\"\"\"\n",
    "    setosa_probability= classes_prob[0]\n",
    "    versicolor_probability = classes_prob[1]\n",
    "    virginica_probability = classes_prob[2]\n",
    "    return - (versicolor_probability * math.log(versicolor_probability) + \n",
    "                  setosa_probability * math.log(setosa_probability) + \n",
    "                  virginica_probability * math.log(virginica_probability))\n",
    "\n",
    "root_entropy = entropy(y)\n",
    "print(f\"{root_entropy=}\")\n",
    "\n",
    "#6b\n",
    "\n",
    "mean_sepal_width = np.mean(X[:, 1])\n",
    "l0_mask = X[:,1] <= mean_sepal_width\n",
    "l0 = X[l0_mask]\n",
    "y0 = y[l0_mask]\n",
    "l1_mask = X[:,1] > mean_sepal_width\n",
    "l1 = X[l1_mask]\n",
    "y1 = y[l1_mask]\n",
    "assert l0.shape[0] + l1.shape[0] == X.shape[0], \"dimension mismatch\"\n",
    "\n",
    "\n",
    "information_gain = root_entropy - ((len(y0)/len(y)) *  entropy(y0)  + (len(y1)/len(y)) * entropy(y1))\n",
    "print(f\"{information_gain=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77db126-2f7a-4352-a829-6fe2a7db8b86",
   "metadata": {},
   "source": [
    "# 7. Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "552535e9-188f-4ad6-926d-28f8e6baf727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _digits_dataset:\n",
      "\n",
      "Optical recognition of handwritten digits dataset\n",
      "--------------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 1797\n",
      ":Number of Attributes: 64\n",
      ":Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      ":Missing Attribute Values: None\n",
      ":Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      ":Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Load digits dataset\n",
    "digits = datasets.load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Train-test split (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed)\n",
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c1a59bb7-101e-4214-b13c-e7a7fa85f6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=0.9907407407407407\n",
      "Support vectors for digit 0: 38\n",
      "Support vectors for digit 1: 84\n",
      "Total support vectors: 122\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "import sklearn\n",
    "from collections import Counter\n",
    "\n",
    "#7a\n",
    "svm = SVC(gamma=0.0012, C=0.85 )\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "print(f\"{accuracy=}\")\n",
    "\n",
    "#7c\n",
    "from collections import Counter\n",
    "all_sv = svm.n_support_\n",
    "total_sv = sum(all_sv)\n",
    "\n",
    "support_indices = svm.support_\n",
    "support_labels = y_train[support_indices]\n",
    "mask_01 = (support_labels == 0) | (support_labels == 1)\n",
    "support_labels_01 = support_labels[mask_01]\n",
    "counts = Counter(support_labels_01)\n",
    "\n",
    "print(f\"Support vectors for digit 0: {counts[0]}\")\n",
    "print(f\"Support vectors for digit 1: {counts[1]}\")\n",
    "print(f\"Total support vectors: {counts[0] + counts[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c354dce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#7d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "support_vectors = X_train[support_indices]\n",
    "dual_coefs = np.abs(svm.dual_coef_[0])  # shape (n_support_vectors,)\n",
    "\n",
    "# Filter for class 0 and 1 support vectors\n",
    "filtered_sv = support_vectors[mask_01]\n",
    "filtered_duals = dual_coefs[mask_01]\n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(10, 5))\n",
    "for i, digit in enumerate([0, 1]):\n",
    "    # Mask and sort by influence\n",
    "    mask = (support_labels_01 == digit)\n",
    "    top_indices = np.argsort(filtered_duals[mask])[::-1][:4]\n",
    "    top_sv = filtered_sv[mask][top_indices]\n",
    "\n",
    "    # Plot the top 4 support vectors\n",
    "    for j, sv in enumerate(top_sv):\n",
    "        axs[i, j].imshow(sv.reshape(8, 8), cmap='gray')\n",
    "        axs[i, j].axis('off')\n",
    "        axs[i, j].set_title(f'Digit {digit}')\n",
    "\n",
    "plt.suptitle(\"Top 4 Influential Support Vectors for Digits 0 and 1\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c354dce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf.best_params_={'C': 6, 'gamma': 0.0006}\n",
      "Param C: [0.6 0.6 0.6 0.6 0.8 0.8 0.8 0.8 1.0 1.0 1.0 1.0 2.0 2.0 2.0 2.0 3.0 3.0\n",
      " 3.0 3.0 4.0 4.0 4.0 4.0 6.0 6.0 6.0 6.0]\n",
      "Param gamma: [0.0001 0.0006 0.001 0.006 0.0001 0.0006 0.001 0.006 0.0001 0.0006 0.001\n",
      " 0.006 0.0001 0.0006 0.001 0.006 0.0001 0.0006 0.001 0.006 0.0001 0.0006\n",
      " 0.001 0.006 0.0001 0.0006 0.001 0.006]\n",
      "results['mean_test_score'][best_index]=np.float64(0.9744073042401734)\n",
      "clf.best_score_=np.float64(0.9744073042401734)\n",
      "----------------------------------------------\n",
      "So the best params are {'C': 6, 'gamma': 0.0006} and the corresponding is mean cross validation accuracy 0.9744073042401734\n"
     ]
    }
   ],
   "source": [
    "#7e\n",
    "param_grid = {\"gamma\":[0.0001,0.0006,0.001,0.006], \"C\":[0.6,0.8,1,2,3,4,6]}\n",
    "metric = \"accuracy\"\n",
    "clf = sklearn.model_selection.GridSearchCV(estimator=SVC(), param_grid=param_grid, scoring=metric)\n",
    "clf.fit(X,y)\n",
    "print(f\"{clf.best_params_=}\")\n",
    "results = clf.cv_results_\n",
    "best_C = 6\n",
    "best_gamma = 0.0006\n",
    "print(f'Param C: {results[\"param_C\"]}')\n",
    "print(f'Param gamma: {results[\"param_gamma\"]}')\n",
    "best_index = 25\n",
    "assert results[\"param_C\"][best_index] == best_C and results[\"param_gamma\"][best_index] == best_gamma\n",
    "print(f\"{results['mean_test_score'][best_index]=}\")\n",
    "\n",
    "print(f\"{clf.best_score_=}\")\n",
    "print(\"----------------------------------------------\")\n",
    "print(f\"So the best params are {clf.best_params_} and the corresponding is mean cross validation accuracy {clf.best_score_}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
